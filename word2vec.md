# Word2Vec原理
提到语言模型的表示，最容易想到的是one-hot编码，即对词汇总量进行统计并对每一个词汇进行编号，然后用一个词汇总数大小的向量，对应编号位置为1，其余位置都为0，这种方法有着严重的缺点，一是占用内存较大，二是每个词汇都是独立的，没有体现出词汇之间的关系。
word2vec是另一种语言模型的表示方法，他充分考虑到了各个词汇之间的关系，并用较小的向量对词汇进行表示，原理是通过训练将每一个词汇映射到一个向量空间，
常用的训练方法有两种，
